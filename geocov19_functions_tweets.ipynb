{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair os arquivos zip e realizar a leitura dos arquivos JSON descompactados\n",
    "def read_files(zip_dir, json_dir, geo_dir, ids_dir, country_code):\n",
    "    \n",
    "    print(\"Extraindo arquivos...\")\n",
    "    extract_files(zip_dir, json_dir)\n",
    "    \n",
    "    json_files = list_files(json_dir, \".json\")\n",
    "        \n",
    "    total_arquivos = len(json_files)\n",
    "    total_arquivos_processados = 0\n",
    "    total_tweets_validos = 0\n",
    "    \n",
    "    print(str(total_arquivos) + \" arquivo(s) extraídos(s)\")\n",
    "    print(\"Processando arquivo(s) extraído(s)...\")\n",
    "    \n",
    "    # Percorrendo arquivos do diretório\n",
    "    for file in json_files:\n",
    "        \n",
    "        total_arquivos_processados = total_arquivos_processados + 1\n",
    "        \n",
    "        # Lendo o arquivo JSON extraído     \n",
    "        num_linhas = sum(1 for line in open(file))\n",
    "        print(\"Lendo arquivo '\"+get_filename(file)+\" com \"+str(num_linhas)+\" linhas\")\n",
    "        new_tweets = read_tweets(file, country_code)\n",
    "        print(\"Tweets válidos encontrados: \"+str(len(new_tweets)))\n",
    "        total_tweets_validos = total_tweets_validos + len(new_tweets)\n",
    "           \n",
    "        # Criando dataframe de geolocalização de tweets\n",
    "        df_geo = create_df_tweets(new_tweets)   \n",
    "            \n",
    "        # Escrevendo json com com as geolocalizações brasileiras\n",
    "        filename = get_filename(file)\n",
    "        csv_path = geo_dir + os.path.sep + filename\n",
    "        print(\"-> Gerando arquivo json '\"+get_filename(csv_path))\n",
    "        df_geo.to_json(csv_path, orient='records', force_ascii=False)\n",
    "        \n",
    "        # Escrevendo json com \n",
    "        output_file_path = ids_dir + os.path.sep + filename + '_ids.csv'\n",
    "        print(\"-> Gerando arquivo com ids '\"+output_file_path)\n",
    "        df_geo.to_csv(output_file_path, sep=';',encoding='utf-8', index=False, header=False, columns=['tweet_id'])        \n",
    "            \n",
    "        df_geo = None\n",
    "        print(\"Tweets válidos até o momento: \"+str(total_tweets_validos))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extrair os arquivos zip\n",
    "def extract_files(zip_dir, json_dir):\n",
    "    \n",
    "    zips = list_files(zip_dir, \".zip\") \n",
    "    \n",
    "    total_arquivos_processados = 0\n",
    "    total_arquivos = len(zips)\n",
    "    \n",
    "    for file in zips:\n",
    "        \n",
    "        total_arquivos_processados = total_arquivos_processados + 1\n",
    "        \n",
    "        if (is_file_extracted(file, json_dir)):\n",
    "            print(\"-> Arquivo ''\"+get_filename(file)+ \"' já extraído anteriormente\"+\" (\"+str(total_arquivos_processados)+\"/\"+str(total_arquivos)+\")\")\n",
    "        else:\n",
    "            # Extraindo arquivo zip\n",
    "            zip = zipfile.ZipFile(file)\n",
    "            print(\"-> Extraindo arquivo '\"+get_filename(zip.filename)+\"' (\"+str(total_arquivos_processados)+\"/\"+str(total_arquivos)+\")\")\n",
    "            zip.extractall(json_dir)\n",
    "            zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para a criação de um dataframe a partir dos tweets gerados com os atributos desejados\n",
    "def create_df_tweets(tweets):\n",
    "    \n",
    "    #tweet_columns = ['tweet_id','created_at','user_id','geo_source','country_code','state','county','city']\n",
    "    tweet_columns = ['tweet_id','created_at','geo_source','state','city']\n",
    "    df = pd.DataFrame(tweets, columns = tweet_columns)\n",
    "    \n",
    "    # Modificando os tipos de colunas para otimização de espaço\n",
    "    df.tweet_id = df.tweet_id.astype('int64')\n",
    "    df.state = df.state.astype('category')\n",
    "    df.city = df.city.astype('category')\n",
    "    \n",
    "    # Informando valores nulos\n",
    "    df.text = np.nan\n",
    "    df.score = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para retornar a localização do tweet a ser considerada (dentre as várias localizações que podem ter sido informadas)\n",
    "def select_location(tweet, country_code):\n",
    "    \n",
    "    if tweet['geo_source'] == 'coordinates':\n",
    "        return tweet['geo']\n",
    "    if tweet['geo_source'] == 'place':\n",
    "        return tweet['place']\n",
    "    if tweet['geo_source'] == 'user_location':\n",
    "        return tweet['user_location']\n",
    "    if tweet['geo_source'] == 'tweet_text':\n",
    "        for location in tweet['tweet_locations']:\n",
    "            if location['country_code'] == country_code:\n",
    "                return location\n",
    "    else: \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para identificar se o tweet que está sendo lido pertence ao país desejado\n",
    "def is_valid_tweet(tweet, country_code):\n",
    "    \n",
    "    # Verificando preliminarmente se os dados pertencem a outros países diferentes do Brasil\n",
    "    if tweet['geo_source'] == 'geo' and 'country_code' in tweet['geo'] and tweet['geo']['country_code'] != country_code:\n",
    "        return False\n",
    "    if tweet['geo_source'] == 'place' and 'country_code' in tweet['place'] and tweet['place']['country_code'] != country_code:\n",
    "        return False\n",
    "    if tweet['geo_source'] == 'user_location' and 'country_code' in tweet['user_location'] and tweet['user_location']['country_code'] != country_code:\n",
    "        return False\n",
    "    \n",
    "    # Verificando se as informações de cidades e estados são nulas\n",
    "    if tweet['geo_source'] == 'geo' and 'country_code' in tweet['geo'] and tweet['geo']['country_code'] == country_code and 'state' in tweet['geo'] and tweet['geo']['state'] != np.nan and 'city' in tweet['geo'] and tweet['geo']['city'] != np.nan:\n",
    "        return True\n",
    "    if tweet['geo_source'] == 'place' and 'country_code' in tweet['place'] and tweet['place']['country_code'] == country_code and 'state' in tweet['place'] and tweet['place']['state'] != np.nan and 'city' in tweet['place'] and tweet['place']['city'] != np.nan:\n",
    "        return True\n",
    "    if tweet['geo_source'] == 'user_location' and 'country_code' in tweet['user_location'] and tweet['user_location']['country_code'] == country_code and 'state' in tweet['user_location'] and tweet['user_location']['state'] != np.nan  and 'city' in tweet['user_location'] and tweet['user_location']['city'] != np.nan:\n",
    "        return True\n",
    "    \n",
    "    # Caso as informações de localização não estejam presentes nos atributos 'geo', 'place' e 'user_location'\n",
    "    if tweet['geo_source'] == 'tweet_text':\n",
    "        for location in tweet['tweet_locations']:\n",
    "            if location['country_code'] == country_code:\n",
    "                if 'state' in location and location['state'] != np.nan and 'city' in location and location['city'] != np.nan:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para a criação de um novo registro de tweet com atributos desejados dos tweets do arquivo original\n",
    "def create_new_tweet(tweet, country_code):\n",
    "       \n",
    "    new_tweet = {}\n",
    "    \n",
    "    location = select_location(tweet, country_code)\n",
    "    \n",
    "    new_tweet['tweet_id'] = tweet['tweet_id']\n",
    "    new_tweet['created_at'] = tweet['created_at']\n",
    "    #new_tweet['user_id'] = tweet['user_id']\n",
    "    new_tweet['geo_source'] = tweet['geo_source']\n",
    "    new_tweet['country_code'] = (location['country_code'] if 'country_code' in location else None)\n",
    "    new_tweet['state'] = (location['state'] if 'state' in location else None)\n",
    "    #new_tweet['county'] = (location['county'] if 'county' in location else None)\n",
    "    new_tweet['city'] = (location['city'] if 'city' in location else None)\n",
    "    \n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar a leitura de tweets de um país desejado a partir do arquivo JSON\n",
    "def read_tweets(file, country_code):\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "    \n",
    "        # Array de tweets tratados\n",
    "        new_tweets = []\n",
    "\n",
    "        # Realizando leitura iterativa do arquivo (todas as colunas selecionadas)\n",
    "        objects = ijson.items(f, \"\", multiple_values=True)\n",
    "\n",
    "        # Selecionando os tweets desejados\n",
    "        tweets = (item for item in objects if is_valid_tweet(item, country_code))\n",
    "\n",
    "        for tweet in tweets:\n",
    "            new_tweets.append(create_new_tweet(tweet, country_code))\n",
    "            \n",
    "    return new_tweets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_extracted(file, dir):\n",
    "    \n",
    "    filename = get_filename(file)\n",
    "    filename = filename.replace(\".zip\",\".json\")\n",
    "    filename = dir + os.path.sep + filename\n",
    "    return os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir, type):\n",
    "    \n",
    "    caminhos = [os.path.join(dir, nome) for nome in os.listdir(dir)]\n",
    "    arquivos = [arq for arq in caminhos if os.path.isfile(arq)]\n",
    "    valid_files = [arq for arq in arquivos if arq.lower().endswith(type)]\n",
    "                \n",
    "    return sorted(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file):\n",
    "    \n",
    "    split = file.split(os.path.sep)\n",
    "    size = len(split)\n",
    "    filename = split[size-1]\n",
    "    return filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
