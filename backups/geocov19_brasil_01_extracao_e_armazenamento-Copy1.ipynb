{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimentos de tweets brasileiros nos períodos anterior e inicial à pandemia de Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de dados brasileiros e armazenamento no banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Base GeoCoV19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O artigo GeoCov19 disponibiliza uma base de dados contendo mais de **524 milhões de registros de tweets**, em **62 línguas**, que fazem referência à Covid-19. Deste total, **94% do conjunto de dados estão geolocalizados**.\n",
    "\n",
    "Para a inclusão de informações de geolocalização foram consideradas informações de localização **providas pelo próprio Twitter (com diferentes níveis de confiabilidade) e localizações extraídas a partir dos textos dos tweets (topônimos)**. A extração a partir dos textos considera qualquer menção a algum local como uma informação de localização válida.\n",
    "\n",
    "Em atendimento à políticas do Twitter, **os dados disponibilizados não possuem os textos dos tweets**. O autor disponibiliza uma **ferramenta para obtenção do conteúdo integral dos tweets** através de seus \"ids\", procedimento conhecido como *hydrate*.\n",
    "\n",
    "Os dados coletados tem por objetivo capacitar comunidades de pesquisa a avaliar como as sociedades estão lidando coletivamente com a crise do Covid-19, desenvolver métodos para identificar notícias falsas, entender lacunas de conhecimento, contruir modelos de previsão, desenvolver alertas à doenças, entre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extração, seleção e análise de sentimentos de **tweets brasileiros** durante o período de **01/02/2020 a 01/05/2020**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informações Técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estrutura do arquivo JSON contendo tweets e informações de localização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **tweet_id**: it represents the Twitter provided id of a tweet\n",
    "\n",
    "2. **created_at**: it represents the Twitter provided \"created_at\" date and time in UTC \n",
    "\n",
    "3. **user_id**: it represents the Twitter provided user id\n",
    "\n",
    "4. **geo_source**: this field shows one of the four values: (i) coordinates, (ii) place, (iii) user_location, or (iv) tweet_text. The value depends on the availability of these fields. The remaining keys can have the following location_json inside them: {\"country_code\":\"us\",\"state\":\"California\",\"county\":\"San Francisco\",\"city\":\"San Francisco\"}.\n",
    "\n",
    "5. **user_location**: It can have a \"location_json\" as described above or an empty JSON {}. This field uses the \"location\" profile meta-data of a Twitter user and represents the user declared location in the text format. We resolve the text to a location. \n",
    "\n",
    "6. **geo**: represents the \"geo\" field provided by Twitter. We resolve the provided latitude and longitude values to locations. It can have a \"location_json\" as described above or an empty JSON {}.\n",
    "\n",
    "7. **tweet_locations**: This field can have an array of \"location_json\" as described above [location_json1, location_json2] or an empty array []. This field uses the tweet content to find toponyms. \n",
    "\n",
    "8. **place**: It can have a \"location_json\" described above or an empty JSON {}. It represents the Twitter-provided \"place\" field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução Técnica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi implementada uma solução para a **leitura e seleção de tweets, a partir dos arquivos de dados providos pelo  GeoCoV19, selecionando registros que faziam referência ao Brasil**. A partir dos registros selecionados foi desenvolvida uma solução para análise de resultados obtidos a partir dos sentimentos das cidades com **scores** mais negativos e mais positivos.\n",
    "\n",
    "Os dados foram disponibilizados em arquivos diários compactados no formato \"zip\", totalizando **90 arquivos**. A solução realiza a descompactação de cada um desses arquivos (que possuem formato JSON) selecionando os registros desejados e criando um novo arquivo JSON com esses registros. \n",
    "\n",
    "Os arquivos disponibilizados possuem informações de **geolocalização dos tweets** (citadas no item anterior) e também um **atributo informando quais destas localizações fornecidas é a mais precisa** (atributo \"geo_source\"). Através deste atributo selecionamos os registros desejados (contendo a coluna country_code = \"br\") durante a leitura dos arquivos JSON.\n",
    "\n",
    "Os registros selecionados foram armazenados em um banco de dados **MongoDB**. Em um segundo momento, foram realizados os *hydrates* desses tweets, a partir de seus *ids*, utilizando a ferramenta **Twarc**. Os textos foram traduzidos com a utilização da ferramenta **Googletrans** para que pudessem ter seus sentimentos analisados, com a utilização do **Vader Sentiment**, ferramenta desenvolvida especialmente para a utilização em textos de redes sociais.\n",
    "\n",
    "Por último, os resultados dos sentimentos obtidos foram analisados com a utilização de recursos das bibliotecas **Pandas e Matplotlib**, para geração de gráficos e **NTLK** e **Spacy**, para processamento de linguagem natural, entre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Importações gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis utilizadas na extração de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório base\n",
    "#home = '/home/mario/Arquivos'\n",
    "home = '/media/mario/Dados2/Arquivos'\n",
    "\n",
    "# Diretório dos arquivos zip \n",
    "zip_dir = home + '/input/geo'\n",
    "\n",
    "# Diretório de descompactação dos arquivos zip (arquivos json)\n",
    "json_dir = home + '/output/json'\n",
    "\n",
    "# Diretório dos arquivos json com geolocalizações brasileiras processados a partir dos zips descompactados\n",
    "geo_dir = home + '/output/geo'\n",
    "\n",
    "# Diretório dos arquivos de ids\n",
    "ids_dir = home + '/output/ids'\n",
    "\n",
    "# Diretório de downloads realizados pelo Twarc\n",
    "downloads_dir = home + '/downloads'\n",
    "\n",
    "# Diretório de tweets com textos\n",
    "tweets_dir = home + '/output/tweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conexão ao banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando estrutura do banco de dados\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Conexão com o servidor do MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "# Conexão com a base de dados do mongoDB\n",
    "db = client.SpedDB\n",
    "\n",
    "# Coleção onde serão inseridos os dados\n",
    "collection = db.tweets_brasil_test2\n",
    "#collection = db.tweets_brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Realização da extração de dados brasileiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para extração de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extração dos arquivos zip e realização da leitura dos arquivos JSON descompactados\n",
    "def read_files(zip_dir, json_dir, geo_dir, ids_dir, country_code):\n",
    "    \n",
    "    print(\"Extraindo arquivos...\")\n",
    "    extract_files(zip_dir, json_dir)\n",
    "    \n",
    "    json_files = list_files(json_dir, \".json\")\n",
    "        \n",
    "    total_arquivos = len(json_files)\n",
    "    total_arquivos_processados = 0\n",
    "    total_tweets_validos = 0\n",
    "    \n",
    "    print(str(total_arquivos) + \" arquivo(s) extraídos(s)\")\n",
    "    print(\"Processando arquivo(s) extraído(s)...\")\n",
    "    \n",
    "    # Percorrendo arquivos do diretório\n",
    "    for file in json_files:\n",
    "        \n",
    "        total_arquivos_processados = total_arquivos_processados + 1\n",
    "        \n",
    "        # Lendo o arquivo JSON extraído     \n",
    "        num_linhas = sum(1 for line in open(file))\n",
    "        print(\"Lendo arquivo '\"+get_filename(file)+\" com \"+str(num_linhas)+\" linhas\")\n",
    "        new_tweets = read_tweets(file, country_code)\n",
    "        print(\"Tweets válidos encontrados: \"+str(len(new_tweets)))\n",
    "        total_tweets_validos = total_tweets_validos + len(new_tweets)\n",
    "           \n",
    "        # Criando dataframe de geolocalização de tweets\n",
    "        df_geo = create_df_tweets(new_tweets)   \n",
    "            \n",
    "        # Escrevendo json com com as geolocalizações brasileiras\n",
    "        filename = get_filename(file)\n",
    "        csv_path = geo_dir + os.path.sep + filename\n",
    "        print(\"-> Gerando arquivo json '\"+get_filename(csv_path))\n",
    "        df_geo.to_json(csv_path, orient='records', force_ascii=False)\n",
    "        \n",
    "        # Escrevendo json com \n",
    "        output_file_path = ids_dir + os.path.sep + filename + '_ids.csv'\n",
    "        print(\"-> Gerando arquivo com ids '\"+output_file_path)\n",
    "        df_geo.to_csv(output_file_path, sep=';',encoding='utf-8', index=False, header=False, columns=['tweet_id'])        \n",
    "            \n",
    "        df_geo = None\n",
    "        print(\"Tweets válidos até o momento: \"+str(total_tweets_validos))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extração dos arquivos zip\n",
    "def extract_files(zip_dir, json_dir):\n",
    "    \n",
    "    zips = list_files(zip_dir, \".zip\") \n",
    "    \n",
    "    total_arquivos_processados = 0\n",
    "    total_arquivos = len(zips)\n",
    "    \n",
    "    for file in zips:\n",
    "        \n",
    "        total_arquivos_processados = total_arquivos_processados + 1\n",
    "        \n",
    "        if (is_file_extracted(file, json_dir)):\n",
    "            print(\"-> Arquivo ''\"+get_filename(file)+ \"' já extraído anteriormente\"+\" (\"+str(total_arquivos_processados)+\"/\"+str(total_arquivos)+\")\")\n",
    "        else:\n",
    "            # Extraindo arquivo zip\n",
    "            zip = zipfile.ZipFile(file)\n",
    "            print(\"-> Extraindo arquivo '\"+get_filename(zip.filename)+\"' (\"+str(total_arquivos_processados)+\"/\"+str(total_arquivos)+\")\")\n",
    "            zip.extractall(json_dir)\n",
    "            zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para a criação de um dataframe a partir dos tweets gerados com os atributos desejados\n",
    "def create_df_tweets(tweets):\n",
    "    \n",
    "    tweet_columns = ['tweet_id','created_at','geo_source','state','city']\n",
    "    df = pd.DataFrame(tweets, columns = tweet_columns)\n",
    "    \n",
    "    # Modificando os tipos de colunas para otimização de espaço\n",
    "    df.tweet_id = df.tweet_id.astype('int64')\n",
    "    df.state = df.state.astype('category')\n",
    "    df.city = df.city.astype('category')\n",
    "    \n",
    "    # Informando valores nulos\n",
    "    df.text = np.nan\n",
    "    df.score = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para retornar a localização do tweet a ser considerada (dentre as várias localizações que podem ter sido informadas)\n",
    "def select_location(tweet, country_code):\n",
    "    \n",
    "    if tweet['geo_source'] == 'coordinates':\n",
    "        return tweet['geo']\n",
    "    if tweet['geo_source'] == 'place':\n",
    "        return tweet['place']\n",
    "    if tweet['geo_source'] == 'user_location':\n",
    "        return tweet['user_location']\n",
    "    if tweet['geo_source'] == 'tweet_text':\n",
    "        for location in tweet['tweet_locations']:\n",
    "            if location['country_code'] == country_code:\n",
    "                return location\n",
    "    else: \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para identificar se o tweet que está sendo lido pertence ao país desejado\n",
    "def is_valid_tweet(tweet, country_code):\n",
    "    \n",
    "    # Verificando preliminarmente se os dados pertencem a outros países diferentes do Brasil\n",
    "    if tweet['geo_source'] == 'geo' and 'country_code' in tweet['geo'] and tweet['geo']['country_code'] != country_code:\n",
    "        return False\n",
    "    if tweet['geo_source'] == 'place' and 'country_code' in tweet['place'] and tweet['place']['country_code'] != country_code:\n",
    "        return False\n",
    "    if tweet['geo_source'] == 'user_location' and 'country_code' in tweet['user_location'] and tweet['user_location']['country_code'] != country_code:\n",
    "        return False\n",
    "    \n",
    "    # Verificando se as informações de cidades e estados são nulas\n",
    "    if tweet['geo_source'] == 'geo' and 'country_code' in tweet['geo'] and tweet['geo']['country_code'] == country_code and 'state' in tweet['geo'] and tweet['geo']['state'] != np.nan and 'city' in tweet['geo'] and tweet['geo']['city'] != np.nan:\n",
    "        return True\n",
    "    if tweet['geo_source'] == 'place' and 'country_code' in tweet['place'] and tweet['place']['country_code'] == country_code and 'state' in tweet['place'] and tweet['place']['state'] != np.nan and 'city' in tweet['place'] and tweet['place']['city'] != np.nan:\n",
    "        return True\n",
    "    if tweet['geo_source'] == 'user_location' and 'country_code' in tweet['user_location'] and tweet['user_location']['country_code'] == country_code and 'state' in tweet['user_location'] and tweet['user_location']['state'] != np.nan  and 'city' in tweet['user_location'] and tweet['user_location']['city'] != np.nan:\n",
    "        return True\n",
    "    \n",
    "    # Caso as informações de localização não estejam presentes nos atributos 'geo', 'place' e 'user_location'\n",
    "    if tweet['geo_source'] == 'tweet_text':\n",
    "        for location in tweet['tweet_locations']:\n",
    "            if location['country_code'] == country_code:\n",
    "                if 'state' in location and location['state'] != np.nan and 'city' in location and location['city'] != np.nan:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para a criação de um novo registro de tweet com atributos desejados dos tweets do arquivo original\n",
    "def create_new_tweet(tweet, country_code):      \n",
    "    new_tweet = {}    \n",
    "    location = select_location(tweet, country_code)    \n",
    "    new_tweet['tweet_id'] = tweet['tweet_id']\n",
    "    new_tweet['created_at'] = tweet['created_at']\n",
    "    new_tweet['geo_source'] = tweet['geo_source']\n",
    "    new_tweet['country_code'] = (location['country_code'] if 'country_code' in location else None)\n",
    "    new_tweet['state'] = (location['state'] if 'state' in location else None)\n",
    "    new_tweet['city'] = (location['city'] if 'city' in location else None)   \n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar a leitura de tweets de um país desejado a partir do arquivo JSON\n",
    "def read_tweets(file, country_code):\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "    \n",
    "        # Array de tweets tratados\n",
    "        new_tweets = []\n",
    "\n",
    "        # Realizando leitura iterativa do arquivo (todas as colunas selecionadas)\n",
    "        objects = ijson.items(f, \"\", multiple_values=True)\n",
    "\n",
    "        # Selecionando os tweets desejados\n",
    "        tweets = (item for item in objects if is_valid_tweet(item, country_code))\n",
    "\n",
    "        for tweet in tweets:\n",
    "            new_tweets.append(create_new_tweet(tweet, country_code))\n",
    "            \n",
    "    return new_tweets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_extracted(file, dir):\n",
    "    \n",
    "    filename = get_filename(file)\n",
    "    filename = filename.replace(\".zip\",\".json\")\n",
    "    filename = dir + os.path.sep + filename\n",
    "    return os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir, type):\n",
    "    \n",
    "    caminhos = [os.path.join(dir, nome) for nome in os.listdir(dir)]\n",
    "    arquivos = [arq for arq in caminhos if os.path.isfile(arq)]\n",
    "    valid_files = [arq for arq in arquivos if arq.lower().endswith(type)]\n",
    "                \n",
    "    return sorted(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file):\n",
    "    \n",
    "    split = file.split(os.path.sep)\n",
    "    size = len(split)\n",
    "    filename = split[size-1]\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para processamento e limpeza de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mario/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função auxiliar para remoção de caracteres indesejados nos textos dos Tweets\n",
    "chars=[\"\\\"\",\"!\",\"@\",\"#\",\"$\",\"%\",\"&\",\"*\",\"(\",\")\",\"-\",\"_\",\"`\",\"'\",\"{\",\"[\",\"]\",\"}\",\"^\",\"~\",\",\",\".\",\";\",\":\",\"\\\",\",\" \"]\n",
    "\n",
    "def clean_words(words):\n",
    "    \n",
    "    new_words = []    \n",
    "    for word in words:\n",
    "         # Verificando se o caracter pertence ao ASCII\n",
    "        if(all(ord(char) < 128 for char in word)):\n",
    "            for letter in word:\n",
    "                if letter in chars:\n",
    "                    word=word.replace(letter,\"\")\n",
    "        new_words.append(word)            \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função auxiliar para remover urls nos textos dos Tweets\n",
    "def clean_urls(words):   \n",
    "    \n",
    "    new_words = []    \n",
    "    for word in words:              \n",
    "        if (bool(re.match('http', word)) == False) and (bool(re.match('//tco', word)) == False) and (bool(re.match('//t.co', word)) == False) and (bool(re.match('RT', word)) == False):\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover \"stopwords\" dos textos dos Tweets\n",
    "def remove_stopwords(words, language): \n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words(language) + list(punctuation)    \n",
    "    new_words=[]\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza o processo de limpeza do texto\n",
    "def process_tweet(text, language):\n",
    "    words = text.split()\n",
    "    words = clean_words(words)\n",
    "    words = clean_urls(words)\n",
    "    words = remove_stopwords(words, language)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realização da leitura dos arquivos zip e extração dos dados desejados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo arquivos...\n",
      "-> Arquivo ''geo_2020-02-01.zip' já extraído anteriormente (1/2)\n",
      "-> Arquivo ''geo_2020-02-02.zip' já extraído anteriormente (2/2)\n",
      "2 arquivo(s) extraídos(s)\n",
      "Processando arquivo(s) extraído(s)...\n",
      "Lendo arquivo 'geo_2020-02-01.json com 666573 linhas\n",
      "Tweets válidos encontrados: 954\n",
      "-> Gerando arquivo json 'geo_2020-02-01.json\n",
      "-> Gerando arquivo com ids '/media/mario/Dados2/Arquivos/output/ids/geo_2020-02-01.json_ids.csv\n",
      "Tweets válidos até o momento: 954\n",
      "Lendo arquivo 'geo_2020-02-02.json com 1462153 linhas\n",
      "Tweets válidos encontrados: 10798\n",
      "-> Gerando arquivo json 'geo_2020-02-02.json\n",
      "-> Gerando arquivo com ids '/media/mario/Dados2/Arquivos/output/ids/geo_2020-02-02.json_ids.csv\n",
      "Tweets válidos até o momento: 11752\n"
     ]
    }
   ],
   "source": [
    "# Realizando a extração dos arquivos zip\n",
    "read_files(zip_dir, json_dir, geo_dir, ids_dir, 'br')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Armazenamento dos tweets brasileiros no banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para armazenamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para inserção de tweets no banco de dados, a partir de arquivos JSON\n",
    "def create_db(collection, json_files):\n",
    "    \n",
    "    for file in json_files: \n",
    "        print('Criando tweets do arquivo '+file)\n",
    "        with open(file) as json_file: \n",
    "            tweets = json.load(json_file)  \n",
    "            for tweet in tweets:\n",
    "                date = datetime.datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')   \n",
    "                tweet['created_at'] = date\n",
    "                tweet['period'] = str(date.year) + \"_\" + str(date.month).zfill(2)\n",
    "                tweet['text'] = None\n",
    "                tweet['lang'] = None\n",
    "                tweet['score'] = None\n",
    "                collection.insert_one(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criação de índice no banco de dados\n",
    "def create_index(collection, column):\n",
    "    \n",
    "    collection.create_index(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realização do armazenamento de tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando tweets do arquivo /media/mario/Dados2/Arquivos/output/geo/geo_2020-02-01.json\n",
      "Criando tweets do arquivo /media/mario/Dados2/Arquivos/output/geo/geo_2020-02-02.json\n"
     ]
    }
   ],
   "source": [
    "# Lendo arquivos json com registros brasileiros\n",
    "json_files = list_files(geo_dir, \".json\")\n",
    "\n",
    "# Criando banco de dados de tweets com as geolocalizações brasileiras\n",
    "create_db(collection, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando index para outros atributos alvos de selects\n",
    "create_index(collection, 'tweet_id')\n",
    "create_index(collection, 'state')\n",
    "create_index(collection, 'city')\n",
    "create_index(collection, 'lang')\n",
    "create_index(collection, 'geo_source')\n",
    "create_index(collection, 'created_at')\n",
    "create_index(collection, 'polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11752"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets inseridos\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ffc340721bf9fca746904c1'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'period': '2020_02',\n",
       " 'text': None,\n",
       " 'lang': None,\n",
       " 'score': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet como exemplo\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Hydrate de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos arquivos de ids gerados no procedimento de leitura dos arquivos zips, foi realizado o *hydrate* dos tweets, a partir desses ids, utilizando a ferramento Twarc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para atualizações no banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para atualização de registro de um tweet no banco de dados\n",
    "def update_tweets_db(collection, json_files): \n",
    "    \n",
    "    for file in json_files:\n",
    "        print('Atualizando tweets do arquivo '+file)\n",
    "        for line in open(file, 'r'):\n",
    "            tweet = json.loads(line)\n",
    "            collection.update_one({\"tweet_id\": tweet['id']}, {'$set':{\"text\": tweet['full_text'], \"lang\": tweet['lang']}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atualização de textos de tweets no banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a atualização no banco de dados com os textos dos tweets retornados pelo *Twarc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando tweets do arquivo /media/mario/Dados2/Arquivos/downloads/tweets_2020-02-01.json\n",
      "Atualizando tweets do arquivo /media/mario/Dados2/Arquivos/downloads/tweets_2020-02-02.json\n"
     ]
    }
   ],
   "source": [
    "# Listando arquivos json gerados pelo Twarc\n",
    "twarc_files = list_files(downloads_dir, '.json')\n",
    "\n",
    "# Atualizando banco de dados com os atributos Text e Lang a partir dos arquivos Json gerados pelo Twarc\n",
    "update_tweets_db(collection, twarc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8494"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets com textos não nulos\n",
    "collection.count_documents({'text': {'$ne':None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ffc340721bf9fca746904c1'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'period': '2020_02',\n",
       " 'text': 'Director General of Health Services Dr. Anil Jasinghe confirmed the Chinese woman admitted at IDH has recovered and can be discharged by tomorrow #Lka #SriLanka #coronavirus',\n",
       " 'lang': 'en',\n",
       " 'score': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet como exemplo\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fontes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) GeoCoV19: A Dataset of Hundreds of Millions ofMultilingual COVID-19 Tweets with Location Information<br>\n",
    "\n",
    "(2) Paper Info, Statics and Downloads - https://crisisnlp.qcri.org/covid19<br>\n",
    "\n",
    "(3) Pyhton Documentation - https://docs.python.org/3/ \n",
    "\n",
    "(4) Muhammad Imran, Prasenjit Mitra, Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia.\n",
    "\n",
    "(5) How to Generate API Key, Consumer Token, Access Key for Twitter OAuth - https://themepacific.com/how-to-generate-api-key-consumer-token-access-key-for-twitter-oauth/994/\n",
    "\n",
    "(6) Iterative JSON parser with a standard Python iterator interface - https://pypi.org/project/ijson/\n",
    "\n",
    "(7) Make working with large DataFrames easier, at least for your memory - https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4\n",
    "\n",
    "(8) Twitter API Documentation - Tweet Location Metadata - https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/geo-objects\n",
    "\n",
    "(9) Twarc - https://github.com/DocNow/twarc\n",
    "\n",
    "(10) Map Polygon - https://www.keene.edu/campus/maps/tool/\n",
    "\n",
    "(11) MongoBD Documentation - https://docs.mongodb.com/manual/reference/command/count/\n",
    "\n",
    "(12) Free Google Translator API for Pyhton - https://pypi.org/project/googletrans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
