{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de registros brasileiros do conjunto de dados GeoCoV19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo do Artigo GeoCoV19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O artigo GeoCov19 disponibiliza uma base de dados contendo mais de 524 milhões de registros de tweets, em 62 línguas, que fazem referência à Covid-19. Deste total, 94% do conjunto de dados estão geolocalizados.\n",
    "\n",
    "Para a inclusão de informações de geolocalização foram consideradas informações de localização providas pelo próprio Twitter (com diferentes níveis de confiabilidade) e localizações extraídas a partir dos textos dos tweets (topônimos). A extração a partir dos textos considera qualquer menção a algum local como uma informação de localização válida.\n",
    "\n",
    "Em atendimento à políticas do Twitter, os dados disponibilizados não possuem os textos dos tweets. O autor disponibiliza uma ferramenta para obtenção do conteúdo integral dos tweets através de seus \"ids\" (que são disponibilizados na base de dados GeoCov19).\n",
    "\n",
    "Os dados coletados tem por objetivo capacitar comunidades de pesquisa a avaliar como as sociedades estão lidando coletivamente com a crise do Covid-19, desenvolver métodos para identificar notícias falsas, entender lacunas de conhecimento, contruir modelos de previsão, desenvolver alertas à doenças, entre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo da Atividade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrair dos dados de tweets citados acima, registros que fazem referência ao Brasil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informações Técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura do arquivo JSON contendo tweets e informações de localização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **tweet_id**: it represents the Twitter provided id of a tweet\n",
    "\n",
    "2. **created_at**: it represents the Twitter provided \"created_at\" date and time in UTC \n",
    "\n",
    "3. **user_id**: it represents the Twitter provided user id\n",
    "\n",
    "4. **geo_source**: this field shows one of the four values: (i) coordinates, (ii) place, (iii) user_location, or (iv) tweet_text. The value depends on the availability of these fields. The remaining keys can have the following location_json inside them: {\"country_code\":\"us\",\"state\":\"California\",\"county\":\"San Francisco\",\"city\":\"San Francisco\"}.\n",
    "\n",
    "5. **user_location**: It can have a \"location_json\" as described above or an empty JSON {}. This field uses the \"location\" profile meta-data of a Twitter user and represents the user declared location in the text format. We resolve the text to a location. \n",
    "\n",
    "6. **geo**: represents the \"geo\" field provided by Twitter. We resolve the provided latitude and longitude values to locations. It can have a \"location_json\" as described above or an empty JSON {}.\n",
    "\n",
    "7. **tweet_locations**: This field can have an array of \"location_json\" as described above [location_json1, location_json2] or an empty array []. This field uses the tweet content to find toponyms. \n",
    "\n",
    "8. **place**: It can have a \"location_json\" described above or an empty JSON {}. It represents the Twitter-provided \"place\" field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução Técnica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo da Solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi implementada uma solução para a leitura dos registros de tweets a partir dos arquivos de dados providos pelo trabalho GeoCoV19, selecionando registros que fazem referência ao Brasil. \n",
    "\n",
    "Os dados foram disponibilizados em arquivos diários compactados no formato \"zip\", totalizando 90 arquivos. A solução realiza a descompactação de cada um desses arquivos (que possuem formato JSON) selecionando os registros desejados e criando um novo arquivo JSON com esses registros. \n",
    "\n",
    "Os arquivos disponibilizados possuem informações de geolocalização dos tweets (citadas no item anterior) e também um atributo informando quais destas localizações fornecidas é a mais precisa (atributo \"geo_source\"). Através deste atributo selecionamos os registros desejados (contendo a coluna country_code = \"br\") durante a leitura dos arquivos JSON.\n",
    "\n",
    "Os registros de tweets brasileiros selecionados são armazenados em uma banco de dados MongoDB. Em um segundo momento, realizamos o \"hydrate\" desses tweets a partir dos seus ids utilizando a ferramenta Twarc. Por último, realizamos a tradução dos textos para inglês, com a utilização da ferramenta Googletrans e calculamos seus escores de sentimentos, utilizando o Vader Sentiment, ferramenta desenvolvida especialmente para a utilização em textos de redes sociais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mario/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importando módulos internos\n",
    "import geocov19_functions_db as fdb\n",
    "import geocov19_functions_tweets as ftweets\n",
    "import geocov19_functions_text as ftext\n",
    "\n",
    "# Importando módulos externos\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório base\n",
    "#home = '/home/mario/Arquivos'\n",
    "#dados = '/media/mario/Dados1/Arquivos'\n",
    "test = '/media/mario/Dados2/Arquivos'\n",
    "\n",
    "# Diretório dos arquivos zip \n",
    "zip_dir = test + '/input/geo'\n",
    "\n",
    "# Diretório de descompactação dos arquivos zip (arquivos json)\n",
    "json_dir = test + '/output/json'\n",
    "\n",
    "# Diretório dos arquivos json com geolocalizações brasileiras processados a partir dos zips descompactados\n",
    "geo_dir = test + '/output/geo'\n",
    "\n",
    "# Diretório dos arquivos de ids\n",
    "ids_dir = test + '/output/ids'\n",
    "\n",
    "# Diretório de downloads realizados pelo Twarc\n",
    "downloads_dir = test + '/downloads'\n",
    "\n",
    "# Diretório de tweets com textos\n",
    "tweets_dir = test + '/output/tweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexão ao BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando estrutura do banco de dados\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Conexão com o servidor do MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "# Conexão com a base de dados do mongoDB\n",
    "db = client.SpedDB\n",
    "\n",
    "# Coleção onde serão inseridos os dados\n",
    "#collection = db.tweets_brasil_test\n",
    "collection = db.tweets_brasil\n",
    "\n",
    "#print(collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realização da leitura dos arquivos zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo arquivos...\n",
      "-> Arquivo ''geo_2020-02-01.zip' já extraído anteriormente (1/2)\n",
      "-> Arquivo ''geo_2020-02-02.zip' já extraído anteriormente (2/2)\n",
      "2 arquivo(s) extraídos(s)\n",
      "Processando arquivo(s) extraído(s)...\n",
      "Lendo arquivo 'geo_2020-02-01.json com 666573 linhas\n",
      "Tweets válidos encontrados: 954\n",
      "-> Gerando arquivo json 'geo_2020-02-01.json\n",
      "-> Gerando arquivo com ids '/media/mario/Dados2/Arquivos/output/ids/geo_2020-02-01.json_ids.csv\n",
      "Tweets válidos até o momento: 954\n",
      "Lendo arquivo 'geo_2020-02-02.json com 1462153 linhas\n",
      "Tweets válidos encontrados: 10798\n",
      "-> Gerando arquivo json 'geo_2020-02-02.json\n",
      "-> Gerando arquivo com ids '/media/mario/Dados2/Arquivos/output/ids/geo_2020-02-02.json_ids.csv\n",
      "Tweets válidos até o momento: 11752\n"
     ]
    }
   ],
   "source": [
    "# Realizando a extração dos arquivos zip\n",
    "ftweets.read_files(zip_dir, json_dir, geo_dir, ids_dir, 'br')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando informações no BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo arquivos json com registros brasileiros\n",
    "json_files = ftweets.list_files(geo_dir, \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando tweets do arquivo /media/mario/Dados2/Arquivos/output/geo/geo_2020-02-01.json\n",
      "Criando tweets do arquivo /media/mario/Dados2/Arquivos/output/geo/geo_2020-02-02.json\n"
     ]
    }
   ],
   "source": [
    "# Criando banco de dados de tweets com as geolocalizações brasileiras\n",
    "fdb.create_db(collection, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6186812"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets inseridos\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5f501410da596ab0c7238d1c'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'user_id': '1199299942797500423',\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'text': 'Director General of Health Services Dr. Anil Jasinghe confirmed the Chinese woman admitted at IDH has recovered and can be discharged by tomorrow #Lka #SriLanka #coronavirus',\n",
       " 'score': 0.1027,\n",
       " 'lang': 'en',\n",
       " 'period': '2020_02_01'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populando textos dos tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos arquivos de ids gerados no procedimento de leitura dos arquivos zips, foi realizado o \"hydrate\" dos tweets, a partir desses ids, utilizando a ferramento Twarc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atualização do banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a atualização no banco de dados com os textos dos tweets retornados pelo Twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando index para o atributo tweet_id\n",
    "fdb.create_index(collection, 'tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando tweets do arquivo /media/mario/Dados2/Arquivos/downloads/tweets_2020-02-01.json\n",
      "Atualizando tweets do arquivo /media/mario/Dados2/Arquivos/downloads/tweets_2020-02-02.json\n"
     ]
    }
   ],
   "source": [
    "# Listando arquivos json gerados pelo Twarc\n",
    "twarc_files = ftweets.list_files(downloads_dir, '.json')\n",
    "\n",
    "# Atualizando banco de dados com os atributos Text e Lang a partir dos arquivos Json gerados pelo Twarc\n",
    "fdb.update_tweets_db(collection, twarc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando index para outros atributos alvos de selects\n",
    "fdb.create_index(collection, 'state')\n",
    "fdb.create_index(collection, 'city')\n",
    "fdb.create_index(collection, 'lang')\n",
    "fdb.create_index(collection, 'geo_source')\n",
    "fdb.create_index(collection, 'created_at')\n",
    "fdb.create_index(collection, 'polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8494"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets com textos não nulos\n",
    "collection.count_documents({'text': {'$ne':None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5f501410da596ab0c7238d1c'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'user_id': '1199299942797500423',\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'text': 'Director General of Health Services Dr. Anil Jasinghe confirmed the Chinese woman admitted at IDH has recovered and can be discharged by tomorrow #Lka #SriLanka #coronavirus',\n",
       " 'score': 0.1027,\n",
       " 'lang': 'en',\n",
       " 'period': '2020_02_01'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para tradução do texto para inglês utilizando a ferramenta \"Googletrans\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_tweet_text(tweet):\n",
    "    \n",
    "    translated = ''\n",
    "        \n",
    "    text = tweet['text']   \n",
    "    from_lang = tweet['lang']\n",
    "    \n",
    "    # Verificando se o texto do tweet já está escrito na língua inglesa\n",
    "    if (from_lang == 'en') or (from_lang == 'en-US'):\n",
    "        translated = text\n",
    "    else:\n",
    "        translated = translator.translate(text, dest='en').text\n",
    "\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para o cálculo do score de sentimento de um texto utilizando a ferramenta VaderSentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ferramenta VaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_scores(tweets):\n",
    "\n",
    "    for tweet in tweets:\n",
    "                \n",
    "        # Traduzindo o texto para o inglês\n",
    "        translated_text = translate_tweet_text(tweet)\n",
    "        \n",
    "        # Efetuando o cálculo do score de sentimento para o texto\n",
    "        vs = analyzer.polarity_scores(translated_text)\n",
    "        \n",
    "        # Recuperando o score e adicionando em uma nova célula do dataframe\n",
    "        collection.update_one({\"tweet_id\": tweet['tweet_id']}, {'$set':{\"score\": vs['compound']}})      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para o cálculo da intensidade da polaridade de um texto utilizando a ferramenta SenticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Ferramenta SenticNet\n",
    "def calculate_polarities(tweets):\n",
    "    \n",
    "    for tweet in tweets:        \n",
    "        #try:                 \n",
    "            # Traduzindo o texto para o inglês\n",
    "            translated_text = translate_tweet_text(tweet)\n",
    "\n",
    "            # Realizando a limpeza da frase\n",
    "            words = ftext.process_tweet(translated_text, 'english')\n",
    "\n",
    "            # Efetuando o cálculo do score de sentimento para o texto\n",
    "            polarity = calculate_words_polarities(words)\n",
    "\n",
    "            # Recuperando o score e adicionando em uma nova célula do dataframe\n",
    "            # collection.update_one({\"tweet_id\": tweet['tweet_id']},{'$set':{\"polarity\": polarity, \"senticnet_processed\": True}})  \n",
    "        \n",
    "        #except:\n",
    "            #print(\"Skipping id \"+str(tweet['tweet_id']+\" Erro: \"+e))\n",
    "            e = sys.exc_info()[1]\n",
    "         #   print (\"Unexpected error:\", e)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para realizar o cálculo de polaridades do SenticNet (foram considerados para o cálculo somente textos em que no mínimo 50% das palavras tenham tido polaridade retornada pela base de conhecimento do SenticNet em inglês)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senticnet.senticnet import SenticNet\n",
    "\n",
    "def calculate_words_polarities(words):\n",
    "    \n",
    "    if(len(words) > 0):\n",
    "        min_words_count = len(words)/2\n",
    "        not_found_words_count = 0\n",
    "        total_polarity_value = 0\n",
    "        language_code = 'en'\n",
    "\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                sn = SenticNet('en')\n",
    "                polarity_str = sn.polarity_intense(word)\n",
    "                polarity_value = float(polarity_str)      \n",
    "                total_polarity_value = total_polarity_value + polarity_value\n",
    "            except KeyError:\n",
    "                not_found_words_count = not_found_words_count + 1\n",
    "\n",
    "        # O cálculo será considerado somente se ao menos metade das palavras forem encontradas na base de conhecimento do SenticNet\n",
    "        if not_found_words_count > min_words_count:\n",
    "            return None\n",
    "        else:\n",
    "            return total_polarity_value / len(words)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperando tweets da base\n",
    "- Tweets que tenham geolocalização restritas a \"place\" e \"user_location\" (as localizações \"tweet_texts\" foram excluídas)\n",
    "- Tweets que tenham scores (Vader) e polarities (SenticNet) calculados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando tweets da base\n",
    "tweets = collection.find({'text':{'$ne':None}, \"senticnet_processed\": {'$eq':None}, 'score': {'$ne':None}, '$or':[{'geo_source':'place'}, {'geo_source':'user_location'}]}, {'tweet_id':1,'lang':1,'text':1,'_id': 0})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando o cálculo do score de sentimento com o Vader para todos os tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_scores(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando o cálculo da intensidade da polaridade com o SenticNet para todos os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_polarities(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = collection.find({'text':'o mano, agora eu tô com medo da porra desse corona. quero nem sair de casa mais'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5f50165ada596ab0c738500d'), 'tweet_id': 1239630821386043399, 'created_at': datetime.datetime(2020, 3, 16, 19, 13, 13), 'user_id': '764882978665271296', 'geo_source': 'place', 'state': 'São Paulo', 'city': 'Valinhos', 'text': 'o mano, agora eu tô com medo da porra desse corona. quero nem sair de casa mais', 'score': -0.5142, 'lang': 'pt', 'polarity': 0.0001666666666666483, 'senticnet_processed': True}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4bf2d9b5cd0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_polarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-98a9a5ec533a>\u001b[0m in \u001b[0;36mcalculate_polarities\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Traduzindo o texto para o inglês\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtranslated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_tweet_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-64c738e83197>\u001b[0m in \u001b[0;36mtranslate_tweet_text\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# src passed is equal to auto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     76\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     77\u001b[0m                                     token=token)\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRANSLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_service_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw_tkk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtkk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_tkk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "calculate_polarities(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "google_new_transError",
     "evalue": "429 (Too Many Requests) from TTS API. Probable cause: Unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google_trans_new/google_trans_new.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, lang_tgt, lang_src, pronounce)\u001b[0m\n\u001b[1;32m    176\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://www.google.com/sorry/index?continue=https://translate.google.cn/_/TranslateWebserverUi/data/batchexecute&q=EhAoBA1LqXgeAKwLesYbVHBaGIa68_8FIhkA8aeDS_W__oXjDrXE23X6jUWwSOMYMHPbMgFy",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mgoogle_new_transError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0f389cca607a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtranslated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'o mano, agora eu tô com medo da porra desse corona. quero nem sair de casa mais'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlang_tgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/google_trans_new/google_trans_new.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, lang_tgt, lang_src, pronounce)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# Request successful, bad response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mgoogle_new_transError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# Request failed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgoogle_new_transError\u001b[0m: 429 (Too Many Requests) from TTS API. Probable cause: Unknown"
     ]
    }
   ],
   "source": [
    "from google_trans_new import google_translator  \n",
    "  \n",
    "translator = google_translator()  \n",
    "translated_text = translator.translate('o mano, agora eu tô com medo da porra desse corona. quero nem sair de casa mais',lang_tgt='en')  \n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fontes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) GeoCoV19: A Dataset of Hundreds of Millions ofMultilingual COVID-19 Tweets with Location Information<br>\n",
    "\n",
    "(2) Paper Info, Statics and Downloads - https://crisisnlp.qcri.org/covid19<br>\n",
    "\n",
    "(3) Pyhton Documentation - https://docs.python.org/3/ \n",
    "\n",
    "(4) Muhammad Imran, Prasenjit Mitra, Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia.\n",
    "\n",
    "(5) How to Generate API Key, Consumer Token, Access Key for Twitter OAuth - https://themepacific.com/how-to-generate-api-key-consumer-token-access-key-for-twitter-oauth/994/\n",
    "\n",
    "(6) Iterative JSON parser with a standard Python iterator interface - https://pypi.org/project/ijson/\n",
    "\n",
    "(7) Make working with large DataFrames easier, at least for your memory - https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4\n",
    "\n",
    "(8) Twitter API Documentation - Tweet Location Metadata - https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/geo-objects\n",
    "\n",
    "(9) Twarc - https://github.com/DocNow/twarc\n",
    "\n",
    "(10) Map Polygon - https://www.keene.edu/campus/maps/tool/\n",
    "\n",
    "(11) MongoBD Documentation - https://docs.mongodb.com/manual/reference/command/count/\n",
    "\n",
    "(12) Free Google Translator API for Pyhton - https://pypi.org/project/googletrans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
