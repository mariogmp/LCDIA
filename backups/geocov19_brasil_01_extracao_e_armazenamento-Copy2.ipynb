{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimentos de tweets brasileiros nos períodos anterior e inicial da pandemia de Covid-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Extração de dados brasileiros e armazenamento em banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Base de Dados GeoCoV19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **GeoCoV19** (1) disponibiliza uma base de dados com ***tweets*** que fazem referência à **pandemia da Covid-19**. Esta base contêm mais de **524 milhões de registros** em **62 línguas**, no intervalo de **01/02/2020 a 01/05/2020**. Deste total, **94% possuem informações de localizações**.\n",
    "\n",
    "Para a inclusão das localizações foram consideradas **informações providas pelo próprio Twitter e localizações extraídas a partir dos textos dos *tweets* (topônimos)**. A extração a partir dos textos considera qualquer menção a algum local como uma informação de localização válida.\n",
    "\n",
    "Em atendimento aos termos de utilização da *API* do Twitter (2), **os dados disponibilizados não possuem textos de usuários.** O trabalho sugere a utilização de ferramentas para **obtenção do conteúdo integral dos *tweets* através de seus *ids***, procedimento conhecido como *hydrate*.\n",
    "\n",
    "Os dados coletados tem por objetivo capacitar comunidades de pesquisa a avaliar como as sociedades estão lidando coletivamente com a crise do Covid-19, desenvolver métodos para identificar notícias falsas, entender lacunas de conhecimento, contruir modelos de previsão, desenvolver alertas à doenças, entre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoCoV19 - Dados Brasileiros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trabalho utiliza a base de dados GeoCoV19 para **extrair, selecionar e analisar sentimentos de *tweets* com localizações brasileiras**. Esses *tweets*, correspondentes a um intervalo de 91 dias, compreende, na maioria das cidades, à **períodos anteriores e iniciais da pandemia de Covid-19 no Brasil**, sendo dividido neste trabalho em três períodos: **antes do primeiro caso**, **após o primeiro caso** e **após à primeira morte**, dentro do intervalo da base de dados trabalhada.\n",
    "\n",
    "Após estes procedimentos, este trabalho apresenta os **resultados das análises de sentimentos** de uma forma consolidada, considerando **cidades que obtiveram as menores e maiores médias de sentimentos**, nos períodos analisados. Busca-se, nestas análises, investigar a **atenção e o comportamento dos usuários do Twitter** em relação à acontecimentos relacionados à evolução da pandemia da Covid-19 no Brasil. \n",
    "\n",
    "Este *notebook*, primeiro entre quatro, desenvolvidos neste trabalho, trata do procedimento de **extração de dados brasileiros** da base GeoCov19. Os *notebooks* posteriores tratam, respectivamente, da seleção dos *tweets* extraídos, das análises de sentimentos desses *tweets* e dos resultados obtidos acerca destas análises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Solução para extração de dados brasileiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados foram disponibilizados pela base de dados **GeoCoV19** em arquivos diários, no formato *JSON*, compactados no formato *zip*, totalizando **91 arquivos** (3). A solução implementada realiza a **descompactação de cada um desses arquivos, selecionando os registros brasileiros e criando novos arquivos *JSON*** com esses registros.\n",
    "\n",
    "Os arquivos disponibilizados possuem **informações de localizações dos tweets** (providas pelos atributos *geo*, *place*, *user_location* e *tweet_text*, como detalharemos mais adiante) e também um **atributo informando quais destas localizações foi considerada** (informada pelo atributo *geo_source*). Através deste atributo selecionamos os registros desejados (considerando atributos *geo_source* com a coluna **country_code = \"br\"**) durante o processamento dos arquivos *JSON*.\n",
    "\n",
    "Os registros selecionados foram armazenados em um banco de dados **MongoDB**. Em um segundo momento, foram realizados os ***hydrates*** desses *tweets*, utilizando a ferramenta **Twarc**, para que pudessem ser obtidos os **textos dos usuários**. Após a realização deste procedimento, o banco de dados foi atualizado contendo os textos desses *tweets* e outras informações relevantes para as análises realizadas neste trabalho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informações Técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta seção detalha os atributos do registro de *tweet* no formato *JSON* disponibilizado pela base de dados GeoCov19. São estes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **tweet_id**: it represents the Twitter provided id of a tweet\n",
    "\n",
    "2. **created_at**: it represents the Twitter provided \"created_at\" date and time in UTC \n",
    "\n",
    "3. **user_id**: it represents the Twitter provided user id\n",
    "\n",
    "4. **geo_source**: this field shows one of the four values: (i) coordinates, (ii) place, (iii) user_location, or (iv) tweet_text. The value depends on the availability of these fields. The remaining keys can have the following location_json inside them: {\"country_code\":\"us\",\"state\":\"California\",\"county\":\"San Francisco\",\"city\":\"San Francisco\"}.\n",
    "\n",
    "5. **user_location**: It can have a \"location_json\" as described above or an empty JSON {}. This field uses the \"location\" profile meta-data of a Twitter user and represents the user declared location in the text format. We resolve the text to a location. \n",
    "\n",
    "6. **geo**: represents the \"geo\" field provided by Twitter. We resolve the provided latitude and longitude values to locations. It can have a \"location_json\" as described above or an empty JSON {}.\n",
    "\n",
    "7. **tweet_locations**: This field can have an array of \"location_json\" as described above [location_json1, location_json2] or an empty array []. This field uses the tweet content to find toponyms. \n",
    "\n",
    "8. **place**: It can have a \"location_json\" described above or an empty JSON {}. It represents the Twitter-provided \"place\" field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A imagem a seguir ilustra um registro de tweet provido pelo GeoCov19, contendo os atributos citados anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/arq_output_json.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Importações gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurações iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variáveis utilizadas na extração de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na seção a seguir são definidas variáveis que representam os diretórios utilizados nos procedimentos de extração dos registros de *tweets* do GeoCoV19. Essa estrutura é ilustradas na imagem a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/diretorios.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumidamente, o diretório de ***input*** refere-se a arquivos da base de dados GeoCov19, o diretório de ***output*** refere-se a arquivos gerados durante o processo de extração dos registros e o diretório ***downloads*** refere-se a arquivos contendo registros completos de *tweets* gerados nos procedimentos de *hydrate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório base\n",
    "#home = '/home/mario/Arquivos'\n",
    "home = '/media/mario/Dados1/Arquivos'\n",
    "\n",
    "# Diretório dos arquivos zip \n",
    "zip_dir = home + '/input/geo'\n",
    "\n",
    "# Diretório de descompactação dos arquivos zip (arquivos json)\n",
    "json_dir = home + '/output/json'\n",
    "\n",
    "# Diretório dos arquivos json com geolocalizações brasileiras processados a partir dos zips descompactados\n",
    "geo_dir = home + '/output/geo'\n",
    "\n",
    "# Diretório dos arquivos de ids\n",
    "ids_dir = home + '/output/ids'\n",
    "\n",
    "# Diretório de downloads realizados pelo Twarc\n",
    "downloads_dir = home + '/downloads'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Banco de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido ao grande volume de registros de tweets com localizações brasileiras previamente selecionados (mais de 6 milhões, como veremos posteriormente), decidiu-se pelo armazenamento destes registros em um **banco de dados MongoDB** (4).\n",
    "\n",
    "O MongoDB é um banco de dados *NOSQL* e utiliza **documentos no formato *JSON*** para manipulação de dados, mesmo formato utilizado pelos registros de *tweets* processados neste trabalho. Este fator, além de sua versatilidade e performance (4), foram determinantes para sua escolha.\n",
    "\n",
    "A seção a seguir realiza a configuração para acesso ao repositório de registros de *tweets*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando estrutura do banco de dados\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Conexão com o servidor do MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "# Conexão com a base de dados do mongoDB\n",
    "db = client.SpedDB\n",
    "\n",
    "# Coleção onde serão inseridos os dados\n",
    "#collection = db.tweets_brasil_test2\n",
    "collection = db.tweets_brasil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Realização da extração de dados brasileiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para extração de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extração dos arquivos zip e realização da leitura dos arquivos JSON descompactados\n",
    "def read_files(zip_dir, json_dir, geo_dir, ids_dir, country_code):\n",
    "    \n",
    "    print(\"Extraindo arquivos...\")\n",
    "    extract_files(zip_dir, json_dir)\n",
    "    \n",
    "    json_files = list_files(json_dir, \".json\")\n",
    "        \n",
    "    total_arquivos = len(json_files)\n",
    "    total_arquivos_processados = 0\n",
    "    total_tweets_validos = 0\n",
    "    \n",
    "    print(str(total_arquivos) + \" arquivo(s) extraídos(s)\")\n",
    "    print(\"Processando arquivo(s) extraído(s)...\")\n",
    "    \n",
    "    # Percorrendo arquivos do diretório\n",
    "    for file in json_files:\n",
    "        \n",
    "        total_arquivos_processados = total_arquivos_processados + 1\n",
    "        \n",
    "        # Lendo o arquivo JSON extraído     \n",
    "        num_linhas = sum(1 for line in open(file))\n",
    "        print(\"Lendo arquivo '\"+get_filename(file)+\" com \"+str(num_linhas)+\" linhas\")\n",
    "        new_tweets = read_tweets(file, country_code)\n",
    "        print(\"Tweets válidos encontrados: \"+str(len(new_tweets)))\n",
    "        total_tweets_validos = total_tweets_validos + len(new_tweets)\n",
    "           \n",
    "        # Criando dataframe de localização de tweets\n",
    "        df_geo = create_df_tweets(new_tweets)   \n",
    "            \n",
    "        # Escrevendo json com registros de tweets de brasileiros\n",
    "        filename = get_filename(file)\n",
    "        csv_path = geo_dir + os.path.sep + filename\n",
    "        print(\"-> Gerando arquivo json '\"+get_filename(csv_path))\n",
    "        df_geo.to_json(csv_path, orient='records', force_ascii=False)\n",
    "        \n",
    "        # Escrevendo json com registros de ids de tweets brasileiros \n",
    "        output_file_path = ids_dir + os.path.sep + filename + '_ids.csv'\n",
    "        print(\"-> Gerando arquivo com ids '\"+output_file_path)\n",
    "        df_geo.to_csv(output_file_path, sep=';',encoding='utf-8', index=False, header=False, columns=['tweet_id'])        \n",
    "            \n",
    "        df_geo = None\n",
    "        print(\"Tweets válidos até o momento: \"+str(total_tweets_validos))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para extração dos arquivos zip\n",
    "def extract_files(zip_dir, json_dir):\n",
    "    \n",
    "    zips = list_files(zip_dir, \".zip\") \n",
    "    \n",
    "    total_arquivos_processados = 0\n",
    "    total_arquivos = len(zips)\n",
    "    \n",
    "    for file in zips:\n",
    "        \n",
    "        total_arquivos_processados = total_arquivos_processados + 1\n",
    "        \n",
    "        if (is_file_extracted(file, json_dir)):\n",
    "            print(\"-> Arquivo ''\"+get_filename(file)+ \"' já extraído anteriormente\"+\" (\"+str(total_arquivos_processados)+\"/\"+str(total_arquivos)+\")\")\n",
    "        else:\n",
    "            # Extraindo arquivo zip\n",
    "            zip = zipfile.ZipFile(file)\n",
    "            print(\"-> Extraindo arquivo '\"+get_filename(zip.filename)+\"' (\"+str(total_arquivos_processados)+\"/\"+str(total_arquivos)+\")\")\n",
    "            zip.extractall(json_dir)\n",
    "            zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para a criação de um dataframe a partir dos tweets gerados com os atributos desejados\n",
    "def create_df_tweets(tweets):\n",
    "    \n",
    "    tweet_columns = ['tweet_id','created_at','geo_source','state','city']\n",
    "    df = pd.DataFrame(tweets, columns = tweet_columns)\n",
    "    \n",
    "    # Modificando os tipos de colunas para otimização de espaço\n",
    "    df.tweet_id = df.tweet_id.astype('int64')\n",
    "    df.state = df.state.astype('category')\n",
    "    df.city = df.city.astype('category')\n",
    "    \n",
    "    # Informando valores nulos\n",
    "    df.text = np.nan\n",
    "    df.score = np.nan\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para retornar a localização do tweet a ser considerada \n",
    "# (dentre as várias localizações que podem ter sido informadas)\n",
    "def select_location(tweet, country_code):\n",
    "    \n",
    "    if tweet['geo_source'] == 'coordinates':\n",
    "        return tweet['geo']\n",
    "    if tweet['geo_source'] == 'place':\n",
    "        return tweet['place']\n",
    "    if tweet['geo_source'] == 'user_location':\n",
    "        return tweet['user_location']\n",
    "    if tweet['geo_source'] == 'tweet_text':\n",
    "        for location in tweet['tweet_locations']:\n",
    "            if location['country_code'] == country_code:\n",
    "                return location\n",
    "    else: \n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para identificar se o tweet que está sendo lido pertence ao país desejado\n",
    "def is_valid_tweet(tweet, country_code):\n",
    "    \n",
    "    # Verificando preliminarmente se os dados pertencem a outros países diferentes do Brasil\n",
    "    if tweet['geo_source'] == 'geo' and 'country_code' in tweet['geo'] and tweet['geo']['country_code'] != country_code:\n",
    "        return False\n",
    "    if tweet['geo_source'] == 'place' and 'country_code' in tweet['place'] and tweet['place']['country_code'] != country_code:\n",
    "        return False\n",
    "    if tweet['geo_source'] == 'user_location' and 'country_code' in tweet['user_location'] and tweet['user_location']['country_code'] != country_code:\n",
    "        return False\n",
    "    \n",
    "    # Verificando se as informações de cidades e estados são nulas\n",
    "    if tweet['geo_source'] == 'geo' and 'country_code' in tweet['geo'] and tweet['geo']['country_code'] == country_code and 'state' in tweet['geo'] and tweet['geo']['state'] != np.nan and 'city' in tweet['geo'] and tweet['geo']['city'] != np.nan:\n",
    "        return True\n",
    "    if tweet['geo_source'] == 'place' and 'country_code' in tweet['place'] and tweet['place']['country_code'] == country_code and 'state' in tweet['place'] and tweet['place']['state'] != np.nan and 'city' in tweet['place'] and tweet['place']['city'] != np.nan:\n",
    "        return True\n",
    "    if tweet['geo_source'] == 'user_location' and 'country_code' in tweet['user_location'] and tweet['user_location']['country_code'] == country_code and 'state' in tweet['user_location'] and tweet['user_location']['state'] != np.nan  and 'city' in tweet['user_location'] and tweet['user_location']['city'] != np.nan:\n",
    "        return True\n",
    "    \n",
    "    # Caso as informações de localização não estejam presentes nos atributos 'geo', 'place' e 'user_location'\n",
    "    if tweet['geo_source'] == 'tweet_text':\n",
    "        for location in tweet['tweet_locations']:\n",
    "            if location['country_code'] == country_code:\n",
    "                if 'state' in location and location['state'] != np.nan and 'city' in location and location['city'] != np.nan:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para a criação de um novo registro de tweet com atributos desejados dos tweets do arquivo original\n",
    "def create_new_tweet(tweet, country_code):      \n",
    "    new_tweet = {}    \n",
    "    location = select_location(tweet, country_code)    \n",
    "    new_tweet['tweet_id'] = tweet['tweet_id']\n",
    "    new_tweet['created_at'] = tweet['created_at']\n",
    "    new_tweet['geo_source'] = tweet['geo_source']\n",
    "    new_tweet['country_code'] = (location['country_code'] if 'country_code' in location else None)\n",
    "    new_tweet['state'] = (location['state'] if 'state' in location else None)\n",
    "    new_tweet['city'] = (location['city'] if 'city' in location else None)   \n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar a leitura de tweets de um país desejado a partir do arquivo JSON\n",
    "def read_tweets(file, country_code):\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "    \n",
    "        # Array de tweets tratados\n",
    "        new_tweets = []\n",
    "        # Realizando leitura iterativa do arquivo (todas as colunas selecionadas)\n",
    "        objects = ijson.items(f, \"\", multiple_values=True)\n",
    "        # Selecionando os tweets desejados\n",
    "        tweets = (item for item in objects if is_valid_tweet(item, country_code))\n",
    "\n",
    "        for tweet in tweets:\n",
    "            new_tweets.append(create_new_tweet(tweet, country_code))\n",
    "            \n",
    "    return new_tweets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_extracted(file, dir):\n",
    "    \n",
    "    filename = get_filename(file)\n",
    "    filename = filename.replace(\".zip\",\".json\")\n",
    "    filename = dir + os.path.sep + filename\n",
    "    return os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(dir, type):\n",
    "    \n",
    "    caminhos = [os.path.join(dir, nome) for nome in os.listdir(dir)]\n",
    "    arquivos = [arq for arq in caminhos if os.path.isfile(arq)]\n",
    "    valid_files = [arq for arq in arquivos if arq.lower().endswith(type)]\n",
    "                \n",
    "    return sorted(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file):\n",
    "    \n",
    "    split = file.split(os.path.sep)\n",
    "    size = len(split)\n",
    "    filename = split[size-1]\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realização da leitura dos arquivos zip e seleção de registros brasileiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na seção abaixo, é realizada a **seleção dos registros de tweets com localizações brasileiras** a partir dos arquivos providos pela base de dados GeoCoV19. Esta seleção é realizada de acordo com os passos seguintes, ilustradas na imagem a seguir:\n",
    "\n",
    "**1)** Acesso à base de dados do GeoCov19. A imagem ilustra a página de disponibilização contendo opções de downloads. (3);\n",
    "\n",
    "**2)** *Downloads* dos 91 arquivos *JSON* compactados no formato *zip* disponibilizados (na versão *all languages*). A imagem ilsutra o diretório contendo arquivos compactados e, em destaque, o conteúdo do arquivo referente ao dia 01/02/2020;\n",
    "\n",
    "**3)** Descompactação desses arquivos. A imagem ilustra os arquivos *JSON* descompactados e, em destaque, o conteúdo do primeiro registro referente ao dia 01/02/2020, disponibilizado pelo GeoCov19;\n",
    "\n",
    "**4)** Leitura dos arquivos descompactados e seleção dos registros brasileiros criando novos arquivos *JSON*. A imagem ilustra os arquivos contendo os registros brasileiros selecionados. A imagem ilustra esses arquivos e, em destaque, o conteúdo do primeiro registro com localização brasileira referente ao dia 01/02/2020;\n",
    "\n",
    "**5)** Geração de arquivos *CSV* contendo os *ids* dos tweets brasileiros para posterior procedimento de *hydrate*. A imagem ilustra os arquivos gerados e, em destaque, o contéudo dos arquivos contendo *ids* dos *tweets* referentes ao dia 01/02/2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/fluxo_extracao.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na seção abaixo, realiza-se a chamada à função de extração exemplificada nas seções anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo arquivos...\n",
      "-> Extraindo arquivo 'geo_2020-03-31.zip' (1/1)\n",
      "1 arquivo(s) extraídos(s)\n",
      "Processando arquivo(s) extraído(s)...\n",
      "Lendo arquivo 'geo_2020-03-31.json com 9578168 linhas\n",
      "Tweets válidos encontrados: 94878\n",
      "-> Gerando arquivo json 'geo_2020-03-31.json\n",
      "-> Gerando arquivo com ids '/media/mario/Dados2/Arquivos/output/ids/geo_2020-03-31.json_ids.csv\n",
      "Tweets válidos até o momento: 94878\n"
     ]
    }
   ],
   "source": [
    "# Realizando a extração dos arquivos zip\n",
    "read_files(zip_dir, json_dir, geo_dir, ids_dir, 'br')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Armazenamento dos tweets brasileiros no banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções auxiliares para armazenamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para inserção de tweets no banco de dados, a partir de arquivos JSON\n",
    "def create_db(collection, json_files):\n",
    "    \n",
    "    for file in json_files: \n",
    "        print('Criando tweets do arquivo '+file)\n",
    "        with open(file) as json_file: \n",
    "            tweets = json.load(json_file)  \n",
    "            for tweet in tweets:\n",
    "                date = datetime.datetime.strptime(tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y')   \n",
    "                tweet['created_at'] = date\n",
    "                tweet['period'] = str(date.year) + \"_\" + str(date.month).zfill(2)\n",
    "                tweet['text'] = None\n",
    "                tweet['lang'] = None\n",
    "                tweet['score'] = None\n",
    "                collection.insert_one(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criação de índice no banco de dados\n",
    "def create_index(collection, column):\n",
    "    \n",
    "    collection.create_index(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realização do armazenamento de tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na seção abaixo, é realizada a **leitura dos arquivos JSON com localizações brasileiras**, contidas no diretório *geo_dir*. Esses arquivos são utilizados como parâmetro de entrada para a função de **criação dos registros de tweets brasileiros no banco de dados MongoDB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando tweets do arquivo /media/mario/Dados2/Arquivos/output/geo/geo_2020-03-31.json\n"
     ]
    }
   ],
   "source": [
    "# Lendo arquivos json com registros brasileiros\n",
    "json_files = list_files(geo_dir, \".json\")\n",
    "\n",
    "# Criando banco de dados de tweets com as geolocalizações brasileiras\n",
    "create_db(collection, json_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, são criados índices no banco de dados para proporcionar maior agilidade nas consultas por atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando index para outros atributos alvos de selects\n",
    "create_index(collection, 'tweet_id')\n",
    "create_index(collection, 'state')\n",
    "create_index(collection, 'city')\n",
    "create_index(collection, 'lang')\n",
    "create_index(collection, 'geo_source')\n",
    "create_index(collection, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6281690"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets inseridos\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5f501410da596ab0c7238d1c'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'user_id': '1199299942797500423',\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'text': 'Director General of Health Services Dr. Anil Jasinghe confirmed the Chinese woman admitted at IDH has recovered and can be discharged by tomorrow #Lka #SriLanka #coronavirus',\n",
       " 'score': 0.1027,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet como exemplo\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Hydrate de Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos **arquivos de *ids* de *tweets*** gerados no procedimento de **leitura dos arquivos *zip***, foram realizados os procedimentos de ***hydrate*** desses registros. Este procedimento consiste no **download, a partir da base de dados do *Twitter*, dos registros completos relativos a um *tweet***. O objetivo deste passo é obter os **textos de usuários** que não puderam ser disponibilizados pela base de dados GeoCoV19, em atendimento aos termos de utilização do *Twitter*.\n",
    "\n",
    "Este procedimento foi realizado externamente a este *notebook*, via linha de comando, com a utilização da ferramenta ***Twarc*** (4), uma das ferramentas sugeridas pelo artigo da base de dados GeoCov19 para a realização desta tarefa. Para a utilização desta ferramenta é necessária a realização de cadastro específico para utilização da API do *Twitter*, a fim de se obter credenciais para acesso (5).\n",
    "\n",
    "O *Twarc* retorna a saída do comando dentro de um arquivo *JSON* contendo todos os atributos relativos aos *tweets* retornados a partir dos *ids* informados. A imagem a seguir ilustra uma chamada ao procedimento de *hydrate* com a utilização desta ferramenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/twarc_hydrate_01_02_2020.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atualização de textos de tweets no banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir da lista de arquivos retornados, é realizada a **atualização dos registros do banco de dados contendo os textos dos tweets**. Além destes, também realizamos a inserção do **atributo *lang*, referente à língua em que o texto do *tweet* foi escrita**, utilizada posteriormente no refinamento dos tweets selecionados. Este procedimento foi ilustrado no passo 4 da imagem anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para atualização de registro de um tweet no banco de dados\n",
    "def update_tweets_db(collection, json_files): \n",
    "    \n",
    "    for file in json_files:\n",
    "        print('Atualizando tweets do arquivo '+file)\n",
    "        for line in open(file, 'r'):\n",
    "            tweet = json.loads(line)\n",
    "            collection.update_one({\"tweet_id\": tweet['id']}, {'$set':{\"text\": tweet['full_text'], \"lang\": tweet['lang']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando tweets do arquivo /media/mario/Dados1/Arquivos/downloads/tweets_2020-03-31.json\n"
     ]
    }
   ],
   "source": [
    "# Listando arquivos json gerados pelo Twarc\n",
    "twarc_files = list_files(downloads_dir, '.json')\n",
    "\n",
    "# Atualizando banco de dados com os atributos Text e Lang a partir dos arquivos Json gerados pelo Twarc\n",
    "update_tweets_db(collection, twarc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5104854"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets com textos não nulos\n",
    "collection.count_documents({'text': {'$ne':None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5f501410da596ab0c7238d1c'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'user_id': '1199299942797500423',\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'text': 'Director General of Health Services Dr. Anil Jasinghe confirmed the Chinese woman admitted at IDH has recovered and can be discharged by tomorrow #Lka #SriLanka #coronavirus',\n",
       " 'score': 0.1027,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet como exemplo\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O procedimento de *hydrate* abordado nesta seção pode ser resumido nestes passos, ilustrados na imagem a seguir.\n",
    "\n",
    "**1)** Diretório de arquivos contendo *ids* de tweets, pré-requisito para a realização dos procedimentos de  *hydrate*. A imagem ilustra os arquivos *CSV* citados e, em destaque, o conteúdo do arquivo referente ao dia 01/02/2020;\n",
    "\n",
    "**2)** Para cada arquivo contido no diretório, é executando um comando para realização do *hydrate* com a utilização do *Twarc*. A imagem ilustra o comando realizado para registros referentes ao dia 01/02/2020;\n",
    "\n",
    "**3)** Cada comando executado gera como saída arquivos *JSON* contendo o conteúdo completo dos *tweets*. A imagem ilustra os arquivos *JSON* gerados, um para cada dia e, em destaque, os atributos referentes ao primeiro tweet do arquivo referente ao dia 01/02/2020;\n",
    "\n",
    "**4)** Por fim, é realizada a atualização dos registros brasileiros contidos no banco de dados MongoDB inserindo os textos obtidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/fluxo_hydrate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste *notebook* foi abordada a solução implementada para a **extração de dados brasileiros** a partir da base de dados do **GeoCov19**. A partir desta extração, foram necessários a realização de procedimentos de ***hydrate***, que consiste em, a partir da base de dados do Twitter, o download do conteúdo completo dos *tweets* desejados. Este procedimento foi necessário pois o **GeoCov19 não disponibiliza textos de usuários** em atendimento aos termos de utilização da *API* do Twitter. Após a obtenção desses textos, estes foram inseridos na base de dados de **registros de tweets brasileiros** no banco de dados MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fontes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) GeoCoV19: A Dataset of Hundreds of Millions ofMultilingual COVID-19 Tweets with Location Information\n",
    "\n",
    "(2) Twitter API Documentation - https://developer.twitter.com/en/docs\n",
    "\n",
    "(3) Paper Info, Statics and Downloads - https://crisisnlp.qcri.org/covid19\n",
    "\n",
    "(4) MongoBD Documentation - https://docs.mongodb.com/manual/reference/command/count/\n",
    "\n",
    "(5) Pyhton Documentation - https://docs.python.org/3/ \n",
    "\n",
    "(6) Twarc - https://github.com/DocNow/twarc\n",
    "\n",
    "(7) How to Generate API Key, Consumer Token, Access Key for Twitter OAuth - https://themepacific.com/how-to-generate-api-key-consumer-token-access-key-for-twitter-oauth/994/\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "(4) Muhammad Imran, Prasenjit Mitra, Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia.\n",
    "\n",
    "(6) Iterative JSON parser with a standard Python iterator interface - https://pypi.org/project/ijson/\n",
    "\n",
    "(7) Make working with large DataFrames easier, at least for your memory - https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4\n",
    "\n",
    "(10) Map Polygon - https://www.keene.edu/campus/maps/tool/\n",
    "\n",
    "(12) Free Google Translator API for Pyhton - https://pypi.org/project/googletrans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
