{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de registros brasileiros do conjunto de dados GeoCoV19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo do Artigo GeoCoV19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O artigo GeoCov19 disponibiliza uma base de dados contendo mais de 524 milhões de registros de tweets, em 62 línguas, que fazem referência à Covid-19. Deste total, 94% do conjunto de dados estão geolocalizados.\n",
    "\n",
    "Para a inclusão de informações de geolocalização foram consideradas informações de localização providas pelo próprio Twitter (com diferentes níveis de confiabilidade) e localizações extraídas a partir dos textos dos tweets (topônimos). A extração a partir dos textos considera qualquer menção a algum local como uma informação de localização válida.\n",
    "\n",
    "Em atendimento à políticas do Twitter, os dados disponibilizados não possuem os textos dos tweets. O autor disponibiliza uma ferramenta para obtenção do conteúdo integral dos tweets através de seus \"ids\" (que são disponibilizados na base de dados GeoCov19).\n",
    "\n",
    "Os dados coletados tem por objetivo capacitar comunidades de pesquisa a avaliar como as sociedades estão lidando coletivamente com a crise do Covid-19, desenvolver métodos para identificar notícias falsas, entender lacunas de conhecimento, contruir modelos de previsão, desenvolver alertas à doenças, entre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo da Atividade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrair dos dados de tweets citados acima, registros que fazem referência ao Brasil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informações Técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura do arquivo JSON contendo tweets e informações de localização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **tweet_id**: it represents the Twitter provided id of a tweet\n",
    "\n",
    "2. **created_at**: it represents the Twitter provided \"created_at\" date and time in UTC \n",
    "\n",
    "3. **user_id**: it represents the Twitter provided user id\n",
    "\n",
    "4. **geo_source**: this field shows one of the four values: (i) coordinates, (ii) place, (iii) user_location, or (iv) tweet_text. The value depends on the availability of these fields. The remaining keys can have the following location_json inside them: {\"country_code\":\"us\",\"state\":\"California\",\"county\":\"San Francisco\",\"city\":\"San Francisco\"}.\n",
    "\n",
    "5. **user_location**: It can have a \"location_json\" as described above or an empty JSON {}. This field uses the \"location\" profile meta-data of a Twitter user and represents the user declared location in the text format. We resolve the text to a location. \n",
    "\n",
    "6. **geo**: represents the \"geo\" field provided by Twitter. We resolve the provided latitude and longitude values to locations. It can have a \"location_json\" as described above or an empty JSON {}.\n",
    "\n",
    "7. **tweet_locations**: This field can have an array of \"location_json\" as described above [location_json1, location_json2] or an empty array []. This field uses the tweet content to find toponyms. \n",
    "\n",
    "8. **place**: It can have a \"location_json\" described above or an empty JSON {}. It represents the Twitter-provided \"place\" field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução Técnica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumo da Solução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi implementada uma solução para a leitura dos registros de tweets a partir dos arquivos de dados providos pelo trabalho GeoCoV19, selecionando registros que fazem referência ao Brasil. \n",
    "\n",
    "Os dados foram disponibilizados em arquivos diários compactados no formato \"zip\", totalizando 90 arquivos. A solução realiza a descompactação de cada um desses arquivos (que possuem formato JSON) selecionando os registros desejados e criando um novo arquivo JSON com esses registros. \n",
    "\n",
    "Os arquivos disponibilizados possuem informações de geolocalização dos tweets (citadas no item anterior) e também um atributo informando quais destas localizações fornecidas é a mais precisa (atributo \"geo_source\"). Através deste atributo selecionamos os registros desejados (contendo a coluna country_code = \"br\") durante a leitura dos arquivos JSON.\n",
    "\n",
    "Os registros de tweets brasileiros selecionados são armazenados em uma banco de dados MongoDB. Em um segundo momento, realizamos o \"hydrate\" desses tweets a partir dos seus ids utilizando a ferramenta Twarc. Por último, realizamos a tradução dos textos para inglês, com a utilização da ferramenta Googletrans e calculamos seus escores de sentimentos, utilizando o Vader Sentiment, ferramenta desenvolvida especialmente para a utilização em textos de redes sociais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mario/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importando módulos internos\n",
    "import geocov19_functions_db as fdb\n",
    "import geocov19_functions_tweets as ftweets\n",
    "import geocov19_functions_text as ftext\n",
    "\n",
    "# Importando módulos externos\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório base\n",
    "#home = '/home/mario/Arquivos'\n",
    "#dados = '/media/mario/Dados1/Arquivos'\n",
    "test = '/media/mario/Dados2/Arquivos'\n",
    "\n",
    "# Diretório dos arquivos zip \n",
    "zip_dir = test + '/input/geo'\n",
    "\n",
    "# Diretório de descompactação dos arquivos zip (arquivos json)\n",
    "json_dir = test + '/output/json'\n",
    "\n",
    "# Diretório dos arquivos json com geolocalizações brasileiras processados a partir dos zips descompactados\n",
    "geo_dir = test + '/output/geo'\n",
    "\n",
    "# Diretório dos arquivos de ids\n",
    "ids_dir = test + '/output/ids'\n",
    "\n",
    "# Diretório de downloads realizados pelo Twarc\n",
    "downloads_dir = test + '/downloads'\n",
    "\n",
    "# Diretório de tweets com textos\n",
    "tweets_dir = test + '/output/tweets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexão ao BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando estrutura do banco de dados\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Conexão com o servidor do MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "# Conexão com a base de dados do mongoDB\n",
    "db = client.SpedDB\n",
    "\n",
    "# Coleção onde serão inseridos os dados\n",
    "#collection = db.tweets_brasil_test\n",
    "collection = db.tweets_brasils\n",
    "\n",
    "#print(collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realização da leitura dos arquivos zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo arquivos...\n",
      "-> Arquivo ''geo_2020-02-01.zip' já extraído anteriormente (1/2)\n",
      "-> Arquivo ''geo_2020-02-02.zip' já extraído anteriormente (2/2)\n",
      "2 arquivo(s) extraídos(s)\n",
      "Processando arquivo(s) extraído(s)...\n",
      "Lendo arquivo 'geo_2020-02-01.json com 666573 linhas\n",
      "Tweets válidos encontrados: 954\n",
      "-> Gerando arquivo json 'geo_2020-02-01.json\n",
      "-> Gerando arquivo com ids '/media/mario/Dados2/Arquivos/output/ids/geo_2020-02-01.json_ids.csv\n",
      "Tweets válidos até o momento: 954\n",
      "Lendo arquivo 'geo_2020-02-02.json com 1462153 linhas\n",
      "Tweets válidos encontrados: 10798\n",
      "-> Gerando arquivo json 'geo_2020-02-02.json\n",
      "-> Gerando arquivo com ids '/media/mario/Dados2/Arquivos/output/ids/geo_2020-02-02.json_ids.csv\n",
      "Tweets válidos até o momento: 11752\n"
     ]
    }
   ],
   "source": [
    "# Realizando a extração dos arquivos zip\n",
    "ftweets.read_files(zip_dir, json_dir, geo_dir, ids_dir, 'br')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando informações no BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo arquivos json com registros brasileiros\n",
    "json_files = ftweets.list_files(geo_dir, \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando tweets do arquivo /media/mario/Dados2/Arquivos/output/geo/geo_2020-02-01.json\n",
      "Criando tweets do arquivo /media/mario/Dados2/Arquivos/output/geo/geo_2020-02-02.json\n"
     ]
    }
   ],
   "source": [
    "# Criando banco de dados de tweets com as geolocalizações brasileiras\n",
    "fdb.create_db(collection, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6186812"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets inseridos\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5f501410da596ab0c7238d1c'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'user_id': '1199299942797500423',\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'text': 'Director General of Health Services Dr. Anil Jasinghe confirmed the Chinese woman admitted at IDH has recovered and can be discharged by tomorrow #Lka #SriLanka #coronavirus',\n",
       " 'score': 0.1027,\n",
       " 'lang': 'en',\n",
       " 'period': '2020_02_01'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populando textos dos tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos arquivos de ids gerados no procedimento de leitura dos arquivos zips, foi realizado o \"hydrate\" dos tweets, a partir desses ids, utilizando a ferramento Twarc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atualização do banco de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando a atualização no banco de dados com os textos dos tweets retornados pelo Twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando index para o atributo tweet_id\n",
    "fdb.create_index(collection, 'tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando tweets do arquivo /media/mario/Dados2/Arquivos/downloads/tweets_2020-02-01.json\n",
      "Atualizando tweets do arquivo /media/mario/Dados2/Arquivos/downloads/tweets_2020-02-02.json\n"
     ]
    }
   ],
   "source": [
    "# Listando arquivos json gerados pelo Twarc\n",
    "twarc_files = ftweets.list_files(downloads_dir, '.json')\n",
    "\n",
    "# Atualizando banco de dados com os atributos Text e Lang a partir dos arquivos Json gerados pelo Twarc\n",
    "fdb.update_tweets_db(collection, twarc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando index para outros atributos alvos de selects\n",
    "fdb.create_index(collection, 'state')\n",
    "fdb.create_index(collection, 'city')\n",
    "fdb.create_index(collection, 'lang')\n",
    "fdb.create_index(collection, 'geo_source')\n",
    "fdb.create_index(collection, 'created_at')\n",
    "fdb.create_index(collection, 'polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8494"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando quantidade de tweets com textos não nulos\n",
    "collection.count_documents({'text': {'$ne':None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5f501410da596ab0c7238d1c'),\n",
       " 'tweet_id': 1223563091025301507,\n",
       " 'created_at': datetime.datetime(2020, 2, 1, 11, 5, 48),\n",
       " 'user_id': '1199299942797500423',\n",
       " 'geo_source': 'tweet_text',\n",
       " 'state': 'Rio de Janeiro',\n",
       " 'city': 'Rio de Janeiro',\n",
       " 'text': 'Director General of Health Services Dr. Anil Jasinghe confirmed the Chinese woman admitted at IDH has recovered and can be discharged by tomorrow #Lka #SriLanka #coronavirus',\n",
       " 'score': 0.1027,\n",
       " 'lang': 'en',\n",
       " 'period': '2020_02_01'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retornando o primeiro tweet\n",
    "collection.find_one({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para tradução do texto para inglês utilizando a ferramenta \"Googletrans\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_tweet_text(tweet):\n",
    "    \n",
    "    translated = ''\n",
    "        \n",
    "    text = tweet['text']   \n",
    "    from_lang = tweet['lang']\n",
    "    \n",
    "    # Verificando se o texto do tweet já está escrito na língua inglesa\n",
    "    if (from_lang == 'en') or (from_lang == 'en-US'):\n",
    "        translated = text\n",
    "    else:\n",
    "        translated = translator.translate(text, dest='en').text\n",
    "\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para o cálculo do score de sentimento de um texto utilizando a ferramenta VaderSentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ferramenta VaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_scores(tweets):\n",
    "\n",
    "    for tweet in tweets:\n",
    "                \n",
    "        # Traduzindo o texto para o inglês\n",
    "        translated_text = translate_tweet_text(tweet)\n",
    "        \n",
    "        # Efetuando o cálculo do score de sentimento para o texto\n",
    "        vs = analyzer.polarity_scores(translated_text)\n",
    "        \n",
    "        # Recuperando o score e adicionando em uma nova célula do dataframe\n",
    "        collection.update_one({\"tweet_id\": tweet['tweet_id']}, {'$set':{\"score\": vs['compound']}})      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para o cálculo da intensidade da polaridade de um texto utilizando a ferramenta SenticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Ferramenta SenticNet\n",
    "def calculate_polarities(tweets):\n",
    "    \n",
    "    for tweet in tweets:        \n",
    "        try:       \n",
    "            # Traduzindo o texto para o inglês\n",
    "            translated_text = translate_tweet_text(tweet)\n",
    "\n",
    "            # Realizando a limpeza da frase\n",
    "            words = ftext.process_tweet(translated_text, 'english')\n",
    "\n",
    "            # Efetuando o cálculo do score de sentimento para o texto\n",
    "            polarity = calculate_words_polarities(words)\n",
    "\n",
    "            # Recuperando o score e adicionando em uma nova célula do dataframe\n",
    "            collection.update_one({\"tweet_id\": tweet['tweet_id']},{'$set':{\"polarity\": polarity, \"senticnet_processed\": True}})  \n",
    "        \n",
    "        except:\n",
    "            #print(\"Skipping id \"+str(tweet['tweet_id']+\" Erro: \"+e))\n",
    "            e = sys.exc_info()[1]\n",
    "            print (\"Unexpected error:\", e)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from senticnet.senticnet import SenticNet\n",
    "\n",
    "def calculate_words_polarities(words):\n",
    "    \n",
    "    if(len(words) > 0):\n",
    "        min_words_count = len(words)/2\n",
    "        not_found_words_count = 0\n",
    "        total_polarity_value = 0\n",
    "        language_code = 'en'\n",
    "\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            try:\n",
    "                #polarity_value = verify_polarity(word)\n",
    "                sn = SenticNet('en')\n",
    "                polarity_str = sn.polarity_intense(word)\n",
    "                polarity_value = float(polarity_str)      \n",
    "                total_polarity_value = total_polarity_value + polarity_value\n",
    "            except KeyError:\n",
    "                not_found_words_count = not_found_words_count + 1\n",
    "\n",
    "        # O cálculo será considerado somente se ao menos metade das palavras forem encontradas na base de conhecimento do SenticNet\n",
    "        if not_found_words_count > min_words_count:\n",
    "            return None\n",
    "        else:\n",
    "            return total_polarity_value / len(words)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperando tweets da base\n",
    "- Tweets que tenham geolocalização restritas a \"place\" e \"user_location\" (as localizações \"tweet_texts\" foram excluídas)\n",
    "- Tweets que tenham scores (Vader) e polarities (SenticNet) calculados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando tweets da base\n",
    "tweets = collection.find({'text':{'$ne':None}, \"senticnet_processed\": {'$eq':None}, 'score': {'$ne':None}, '$or':[{'geo_source':'place'}, {'geo_source':'user_location'}]}, {'tweet_id':1,'lang':1,'text':1,'_id': 0})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando o cálculo do score de sentimento com o Vader para todos os tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_scores(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando o cálculo da intensidade da polaridade com o SenticNet para todos os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calculate_polarities(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fontes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) GeoCoV19: A Dataset of Hundreds of Millions ofMultilingual COVID-19 Tweets with Location Information<br>\n",
    "\n",
    "(2) Paper Info, Statics and Downloads - https://crisisnlp.qcri.org/covid19<br>\n",
    "\n",
    "(3) Pyhton Documentation - https://docs.python.org/3/ \n",
    "\n",
    "(4) Muhammad Imran, Prasenjit Mitra, Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia.\n",
    "\n",
    "(5) How to Generate API Key, Consumer Token, Access Key for Twitter OAuth - https://themepacific.com/how-to-generate-api-key-consumer-token-access-key-for-twitter-oauth/994/\n",
    "\n",
    "(6) Iterative JSON parser with a standard Python iterator interface - https://pypi.org/project/ijson/\n",
    "\n",
    "(7) Make working with large DataFrames easier, at least for your memory - https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4\n",
    "\n",
    "(8) Twitter API Documentation - Tweet Location Metadata - https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/geo-objects\n",
    "\n",
    "(9) Twarc - https://github.com/DocNow/twarc\n",
    "\n",
    "(10) Map Polygon - https://www.keene.edu/campus/maps/tool/\n",
    "\n",
    "(11) MongoBD Documentation - https://docs.mongodb.com/manual/reference/command/count/\n",
    "\n",
    "(12) Free Google Translator API for Pyhton - https://pypi.org/project/googletrans/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
